{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import sys, getopt\n",
    "import jsonlines\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################\n",
    "# Utility Functions\n",
    "###################\n",
    "\n",
    "def getWithoutAC(s):\n",
    "    s = s[1:-1]\n",
    "    s = s.split(\", \")\n",
    "    withoutAccess = []\n",
    "    for k in s:\n",
    "        if \"[AC]\" not in k:\n",
    "            withoutAccess.append(k)\n",
    "    return \"<|>\".join(withoutAccess)\n",
    "\n",
    "def extractAC(s):\n",
    "    s = s[1:-1]\n",
    "    s = s.split(\", \")\n",
    "    ans = -1\n",
    "    \n",
    "    for k in s:\n",
    "        if \"[AC]\" in k:\n",
    "            match = re.search(r'\\((\\d+)\\)', k)\n",
    "            if match:\n",
    "                number = int(match.group(1))\n",
    "                ans = max(ans, number)\n",
    "    m = {-1: 0, 0: 0, 1: 0, 2: 1, 3: 1}\n",
    "    return m[ans]\n",
    "\n",
    "def parse_data_string(dict_str):\n",
    "    newDict = {}\n",
    "    dict_str = dict_str[1:-1]\n",
    "    dict_str = dict_str.split(',')\n",
    "    for d in dict_str:\n",
    "        newDict[d.split('=')[0].strip()] = d.split('=')[1].strip()\n",
    "    return newDict\n",
    "\n",
    "def getData(fileName):\n",
    "# Read the file\n",
    "    with open(fileName, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Initialize variables\n",
    "    current_ep = None\n",
    "    all_entries = []\n",
    "\n",
    "    # Iterate over the lines in the file\n",
    "    for line in lines:\n",
    "        # If the line starts with 'EP:', it's the start of a new EP block\n",
    "        if line.startswith('EP:'):\n",
    "            current_ep = line.strip().split('EP: ')[1]\n",
    "        # If the line starts with 'ControlFlow:', it's the start of a new data block\n",
    "        elif line.startswith('ControlFlow:'):\n",
    "            current_block = {}\n",
    "            current_block['EP'] = current_ep\n",
    "            current_block['code'] = line.strip().split('ControlFlow: ')[1]\n",
    "            all_entries.append(current_block)\n",
    "        # elif line.startswith('Features:'):\n",
    "        #     newDict = parse_data_string(line.strip().split('Features: ')[1])\n",
    "        #     for k in newDict.keys():\n",
    "        #         current_block[k] = newDict[k]\n",
    "        #     all_entries.append(current_block)\n",
    "\n",
    "    # Convert the list of blocks to a dataframe\n",
    "    df = pd.DataFrame(all_entries)\n",
    "    df[\"label\"] = df[\"code\"].apply(extractAC)\n",
    "    df[\"code\"] = df[\"code\"].apply(getWithoutAC)\n",
    "    return df\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def slidingWindow(path):\n",
    "    window = []\n",
    "    curPath = []\n",
    "    for p in path:\n",
    "        curPath.append(p)\n",
    "        window.append(list(curPath))\n",
    "    return window\n",
    "\n",
    "\n",
    "def decompose(df):\n",
    "    # Create a helper column for chunk grouping\n",
    "    df['group'] = np.where(df['label'] == 0, np.arange(len(df)), df.groupby('EP').cumcount() // 4)\n",
    "\n",
    "    # Group by 'EP', 'label' and 'group', and then merge 'subsequences'\n",
    "    df_agg = df.groupby(['EP', 'label', 'group']).agg({\n",
    "        'code': lambda x: '<PATH_SEP>'.join(x)\n",
    "    })\n",
    "\n",
    "    # Reset the index to get 'EP' and 'label' as columns\n",
    "    df_agg.reset_index(inplace=True)\n",
    "\n",
    "    # Drop the 'group' column\n",
    "    df_agg.drop(columns=['group'], inplace=True)\n",
    "\n",
    "    # If you still want to aggregate 'label' by taking the max value\n",
    "    df_agg['label'] = df_agg.groupby('EP')['label'].transform('max')\n",
    "\n",
    "    return df_agg\n",
    "\n",
    "\n",
    "\n",
    "def shuffle_dataframes(df):\n",
    "    # Concatenate the dataframes\n",
    "    \n",
    "    # Separate the labels from the features\n",
    "    X = df.drop('label', axis=1)\n",
    "    y = df['label']\n",
    "\n",
    "    # Split the data into train and remaining data (test + validation)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Create train, test, and validation dataframes\n",
    "    train_df = pd.concat([X_train, y_train], axis=1)\n",
    "    test_df = pd.concat([X_test, y_test], axis=1)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "# Creating the Training Data for the model\n",
    "def createJsonL(df, fileName):\n",
    "    fileName = fileName.replace(\".txt\", \"\")\n",
    "    json_objects = []\n",
    "\n",
    "    # Group the DataFrame by unique values in the 'EP' column\n",
    "    grouped = df.groupby('EP')\n",
    "\n",
    "    # Iterate over each group\n",
    "    for _, group in grouped:\n",
    "        # Shuffle the group randomly\n",
    "        shuffled_group = group.sample(frac=1, random_state=42)  # Set a random_state for reproducibility\n",
    "\n",
    "        # Keep at most 10 rows in the shuffled group\n",
    "        # shuffled_group = shuffled_group.head(10)\n",
    "\n",
    "        # Iterate over each row in the shuffled group\n",
    "        for _, row in shuffled_group.iterrows():\n",
    "            prompt = []\n",
    "\n",
    "            # Iterate over each column (excluding 'label')\n",
    "            for column in shuffled_group.columns:\n",
    "                if column != 'label':\n",
    "                    prompt.append(str(column) + ' = ' + str(row[column]))\n",
    "\n",
    "            # Create the JSON object\n",
    "            json_object = {\n",
    "                'code': \" \".join(prompt),\n",
    "                'label': int(row['label'])\n",
    "            }\n",
    "\n",
    "            # Append the JSON object to the list\n",
    "            json_objects.append(json_object)\n",
    "\n",
    "    with jsonlines.open(fileName, 'w') as writer:\n",
    "        writer.write_all(json_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = getAggregate(df)\n",
    "def run(argv):\n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv, \"h\", [\"file=\"])\n",
    "    except getopt.GetoptError:\n",
    "        sys.exit(2)\n",
    "\n",
    "    file_name = None\n",
    "\n",
    "    for opt, arg in opts:\n",
    "        if opt == '-h':\n",
    "            print('Example: python pre_process.py --file=<file_path>')\n",
    "            sys.exit()\n",
    "        elif opt == \"--file\":\n",
    "            file_name = arg\n",
    "        else:\n",
    "            print('Example: python pre_process.py --file=<file_path>')\n",
    "            sys.exit()\n",
    "    \n",
    "    if not file_name:\n",
    "        print('Example Usage: python pre_process.py --file=<file_path>')\n",
    "        sys.exit()\n",
    "    \n",
    "    try:\n",
    "        df = getData(file_name)\n",
    "    except:\n",
    "        print(f\"Could not read {file_name}\")\n",
    "    \n",
    "    label_counts = df['label'].value_counts()\n",
    "\n",
    "    print(\"################\\n1 - Protection Required\\n0 - No Protection\\n################\\n\")\n",
    "\n",
    "    print(\"LabelCounts:\\n\", label_counts)\n",
    "    merged_aaAdf = decompose(df)\n",
    "    trainDF, testDF = shuffle_dataframes(merged_df)\n",
    "\n",
    "    label_counts = merged_df['label'].value_counts()\n",
    "    print(\"LabelCounts:\\n\", label_counts)\n",
    "\n",
    "    print(merged_df.head())\n",
    "\n",
    "    createJsonL(trainDF, f'{file_name}_train.jsonl')\n",
    "    createJsonL(testDF, f'{file_name}_test.jsonl')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run(sys.argv[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################\n",
      "1 - Protection Required\n",
      "0 - No Protection\n",
      "################\n",
      "\n",
      "LabelCounts:\n",
      " label\n",
      "1    36207\n",
      "0     9623\n",
      "Name: count, dtype: int64\n",
      "LabelCounts:\n",
      " label\n",
      "1    14903\n",
      "0     5242\n",
      "Name: count, dtype: int64\n",
      "                                 EP  label  \\\n",
      "0  abandonAudioFocus_AudioService_5      0   \n",
      "1  abandonAudioFocus_AudioService_5      0   \n",
      "2  abandonAudioFocus_AudioService_5      0   \n",
      "3  abandonAudioFocus_AudioService_5      0   \n",
      "4  abandonAudioFocus_AudioService_5      0   \n",
      "\n",
      "                                                code  \n",
      "0  [get]: MediaFocusControl var6 = this.mMediaFoc...  \n",
      "1  [get]: MediaFocusControl var6 = this.mMediaFoc...  \n",
      "2  [get]: MediaFocusControl var6 = this.mMediaFoc...  \n",
      "3  [get]: MediaFocusControl var6 = this.mMediaFoc...  \n",
      "4  [get]: MediaFocusControl var6 = this.mMediaFoc...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "file_name = \"data/aosp.txt\"\n",
    "try:\n",
    "    df = getData(file_name)\n",
    "except:\n",
    "    print(f\"Could not read {file_name}\")\n",
    "\n",
    "label_counts = df['label'].value_counts()\n",
    "\n",
    "print(\"################\\n1 - Protection Required\\n0 - No Protection\\n################\\n\")\n",
    "\n",
    "print(\"LabelCounts:\\n\", label_counts)\n",
    "merged_aaAdf = decompose(df)\n",
    "merged_df = merged_aaAdf\n",
    "trainDF, testDF = shuffle_dataframes(merged_df)\n",
    "\n",
    "label_counts = merged_df['label'].value_counts()\n",
    "print(\"LabelCounts:\\n\", label_counts)\n",
    "\n",
    "print(merged_df.head())\n",
    "\n",
    "# createJsonL(trainDF, f'{file_name}_train.jsonl')\n",
    "# createJsonL(testDF, f'{file_name}_test.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'// Method: adjustSuggestedStreamVolume\\n// Service: AudioService_6\\n\\npublic void adjustSuggestedStreamVolume() {\\n    Object var7 = this.mExtVolumeControllerLock;\\n    IAudioPolicyCallback var8 = this.mExtVolumeController;\\n    int var13 = Binder.getCallingUid();\\n    adjustSuggestedStreamVolume(param2,param3,param4,param5,param6,var13);\\n    AudioServiceEvents$VolumeEvent var10 = new AudioServiceEvents$VolumeEvent();\\n    Object var27 = this.mForceControlStreamLock;\\n    boolean var28 = this.mUserSelectedVolumeControlStream;\\n    int var47 = this.mVolumeControlStream;\\n    boolean var73 = isMuteAdjust(param2);\\n    ensureValidStreamType(var51);{ensureValidStreamType(var51);};\\n    [I var75 = mStreamVolumeAlias;\\n    76 = arrayload 75[51];\\n    var78 = param4 and 4;\\n    var80 = param4 and -5;\\n    AudioService$VolumeController var82 = this.mVolumeController;\\n    boolean var84 = var82.suppressAdjustment(var76,var81,var73);\\n    boolean var85 = this.mIsSingleVolume;\\n    var87 = var81 and -5;\\n    var89 = var87 and -17;\\n    adjustStreamVolume(var51,var92,var93,param5,param6,param7);\\n}'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainDF.iloc[0]['code']\n",
    "def generate_java_method(name, code):\n",
    "    # Extract the API name and method name dynamically\n",
    "    api_name, method_name = name.split(\"_\", 1)\n",
    "\n",
    "    # Start the Java code formatting\n",
    "    java_code = []\n",
    "    java_code.append(f\"// Method: {api_name}\")\n",
    "    java_code.append(f\"// Service: {method_name}\\n\")\n",
    "    java_code.append(f\"public void {api_name}() {{\")\n",
    "\n",
    "    # Split and clean up the code\n",
    "    lines = code.split(\"<|>\")\n",
    "    for line in lines:\n",
    "        # Remove [..]: and clean up the lines\n",
    "        cleaned_line = re.sub(r\"\\[.*?\\]: \", \"\", line).strip()\n",
    "        if cleaned_line:\n",
    "            # Ensure only one semicolon is added\n",
    "            if not cleaned_line.endswith(\";\"):\n",
    "                cleaned_line += \";\"\n",
    "            java_code.append(f\"    {cleaned_line}\")\n",
    "\n",
    "    java_code.append(\"}\")\n",
    "    return \"\\n\".join(java_code)\n",
    "\n",
    "\n",
    "s= generate_java_method(trainDF.iloc[0]['EP'],trainDF.iloc[0]['code'])\n",
    "s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['java_code'] = trainDF.apply(lambda row: generate_java_method(row['EP'], row['code']), axis=1)\n",
    "## DROP THE DUPLICATES USING LENGTH\n",
    "trainDF['code_length'] = trainDF['java_code'].map(len)\n",
    "trainDF = (\n",
    "    trainDF.sort_values(by='code_length', ascending=False)  \n",
    "    .drop_duplicates(subset=trainDF.columns[0])             \n",
    "    .reset_index(drop=True)                                 \n",
    ")\n",
    "trainDF = trainDF.drop(columns=['code_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2292, 4)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

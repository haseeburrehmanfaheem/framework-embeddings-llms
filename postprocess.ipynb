{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import csv\n",
    "\n",
    "model = 'llama3.1:70b'\n",
    "num_ctx = 15000\n",
    "\n",
    "\n",
    "checkpoint=\"Salesforce/codet5p-110m-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(checkpoint, trust_remote_code=True).to(\"cuda\")\n",
    "def get_code_embedding(code_snippet):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a given code snippet using a pre-trained model.\n",
    "\n",
    "    Parameters:\n",
    "    - code_snippet (str): The code for which embeddings are to be generated.\n",
    "    - checkpoint (str): The model checkpoint to be used for embedding. Default is Salesforce/codet5p-110m-embedding.\n",
    "    - device (str): Device to run the model on, either 'cuda' for GPU or 'cpu' for CPU. Default is 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Embedding tensor for the input code.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.encode(code_snippet, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        embedding = model(inputs)[0]\n",
    "    \n",
    "    return embedding.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns with valid json  = 48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ApplicationInfo var210 = var207.getApplicationInfo(param5,1024,var19);\\nServiceInfo var229 = new ServiceInfo();\\nApplicationInfo var231 = new ApplicationInfo();\\nvar229.applicationInfo = var231;\\nString var235 = var210.packageName;\\nvar234.packageName = var235;\\nint var237 = var210.uid;\\nvar236.uid = var237;',\n",
       " 'ComponentName var238 = new ComponentName();\\nString var239 = var210.packageName;\\nString var241 = var138.getClassName();\\nIntent var265 = setComponent(var238);',\n",
       " 'ActivityManagerService var276 = this.mAm;\\nboolean var282 = var276.isSingleton(var277,var278,var279,var280);',\n",
       " 'IntentFirewall var421 = var420.mIntentFirewall;\\nComponentName var422 = var408.name;\\nApplicationInfo var423 = var408.appInfo;\\nboolean var425 = var421.checkService(var422,param2,param7,param6,param4,var423);',\n",
       " 'AppOpsManager var438 = this.mAm;\\nAppOpsService var439 = var438.mAppOpsService;']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_file_path = 'AMS_Df_promptsaturday.pkl'\n",
    "# read the serialized DataFrame\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "    AMS_Df = pickle.load(file)\n",
    "AMS_Df.head()\n",
    "AMS_Df['EP'] = AMS_Df['EP'].apply(lambda i: i.split(\"_\")[0])\n",
    "def process_json_answer(json_answer):\n",
    "    global counter\n",
    "    all = []\n",
    "    try:\n",
    "        arrays = json_answer['Sinks']\n",
    "        for i, array in enumerate(arrays, 1):\n",
    "            joined = '\\n'.join(array)\n",
    "            all.append(joined)\n",
    "    except:    \n",
    "        return []\n",
    "    counter += 1\n",
    "    return all\n",
    "\n",
    "# get first row of the DataFrame\n",
    "# test = AMS_Df.iloc[0][\"json_answer\"]\n",
    "# test\n",
    "\n",
    "# Extract each array\n",
    "# arrays = test['Sinks']\n",
    "\n",
    "# for i, array in enumerate(arrays, 1):\n",
    "#     joined = '\\n'.join(array)\n",
    "#     print(f\"Array {i} (Joined):\\n{joined}\\n\")\n",
    "counter = 0\n",
    "# Step 2: Apply to create a new column for joined json_answer\n",
    "AMS_Df['sink_code'] = AMS_Df['json_answer'].apply(process_json_answer)\n",
    "\n",
    "# display the first row first column of the DataFrame\n",
    "print(f\"columns with valid json  = {counter}\")\n",
    "AMS_Df.head()\n",
    "AMS_Df.iloc[0][\"sink_code\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows:   0%|          | 0/192 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows: 100%|██████████| 192/192 [00:02<00:00, 80.62it/s]\n"
     ]
    }
   ],
   "source": [
    "AMS_Df[\"embeddings\"] = None\n",
    "for index, row in tqdm(AMS_Df.iterrows(), total=len(AMS_Df), desc=\"Processing Rows\"):\n",
    "    code_snippets = row[\"sink_code\"]\n",
    "    embeddings = []\n",
    "    for each in code_snippets:\n",
    "        code_embedding = get_code_embedding(each)\n",
    "        embeddings.append(code_embedding)\n",
    "    AMS_Df.at[index, \"embeddings\"] = embeddings\n",
    "\n",
    "# save with pickle not needed since its v.fast\n",
    "# AMS_Df.columns\n",
    "# store using pickle \n",
    "# with open('AMS_Df_embeddings.pkl', 'wb') as file:\n",
    "#     pickle.dump(AMS_Df, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Cosine Similarity # F.cosine_similarity(embedding2, embedding3, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the most similar code(prolly ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Similarities:   0%|          | 0/192 [00:00<?, ?it/s]/tmp/ipykernel_2409107/3436371449.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sim = torch.nn.functional.cosine_similarity(torch.tensor(emb1), torch.tensor(emb2), dim=0).item()\n",
      "Computing Similarities: 100%|██████████| 192/192 [00:17<00:00, 11.25it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "similarities = {}\n",
    "\n",
    "\n",
    "for index1, row1 in tqdm(AMS_Df.iterrows(), total=len(AMS_Df), desc=\"Computing Similarities\"):\n",
    "    ep1_id = row1[\"EP\"]  # name of the EP1\n",
    "    ep1_embeddings = row1[\"embeddings\"] # list of embeddings for the EP1\n",
    "    ep1_sink_code = row1[\"sink_code\"] # list of code snippets for the EP1\n",
    "    \n",
    "    closest_ep = None\n",
    "    max_similarity = -1\n",
    "    similar_sink_pair = None\n",
    "    \n",
    "    for index2, row2 in AMS_Df.iterrows():\n",
    "        if index1 == index2:\n",
    "            continue  \n",
    "        \n",
    "        ep2_id = row2[\"EP\"] # name of the EP2\n",
    "        ep2_embeddings = row2[\"embeddings\"] # list of embeddings for the EP2\n",
    "        ep2_sink_code = row2[\"sink_code\"]  # list of code snippets for the EP2\n",
    "        \n",
    "        # Compute pairwise similarities between all embeddings of EP1 and EP2\n",
    "        for i, emb1 in enumerate(ep1_embeddings):\n",
    "            for j, emb2 in enumerate(ep2_embeddings):\n",
    "                sim = torch.nn.functional.cosine_similarity(torch.tensor(emb1), torch.tensor(emb2), dim=0).item()\n",
    "                if sim > max_similarity:\n",
    "                    max_similarity = sim\n",
    "                    closest_ep = ep2_id\n",
    "                    similar_sink_pair = (ep1_sink_code[i], ep2_sink_code[j])  \n",
    "    \n",
    "    similarities[ep1_id] = {\n",
    "        \"closest_ep\": closest_ep,\n",
    "        \"max_similarity\": max_similarity,\n",
    "        \"similar_sink_pair\": similar_sink_pair\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep, details in similarities.items():\n",
    "    if details['similar_sink_pair'] is None or details['closest_ep'] is None:\n",
    "        print(f\"EP {ep} has no similar EPs.\")\n",
    "    else:\n",
    "        print(f\"EP {ep} is most similar to EP {details['closest_ep']} with similarity {details['max_similarity']:.4f}\")\n",
    "        print(f\"Similar sink codes:\\nEP {ep}: \\n{details['similar_sink_pair'][0]}\\nEP \\n{details['closest_ep']}: \\n{details['similar_sink_pair'][1]}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get >0.8 similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Similarities:   0%|          | 0/192 [00:00<?, ?it/s]/tmp/ipykernel_2409107/4068078957.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sim = torch.nn.functional.cosine_similarity(torch.tensor(emb1), torch.tensor(emb2), dim=0).item()\n",
      "Computing Similarities: 100%|██████████| 192/192 [00:17<00:00, 11.27it/s]\n"
     ]
    }
   ],
   "source": [
    "similarities = {}\n",
    "\n",
    "for index1, row1 in tqdm(AMS_Df.iterrows(), total=len(AMS_Df), desc=\"Computing Similarities\"):\n",
    "    ep1_id = row1[\"EP\"] # name of the EP1\n",
    "    ep1_embeddings = row1[\"embeddings\"] # list of embeddings for the EP1\n",
    "    ep1_sink_code = row1[\"sink_code\"] # list of code snippets for the EP1\n",
    "    \n",
    "    similar_sink_pairs = []  # List to store all similar sink code pairs with similarity > 0.8\n",
    "    \n",
    "    for index2, row2 in AMS_Df.iterrows():\n",
    "        if index1 == index2:\n",
    "            continue  \n",
    "        \n",
    "        ep2_id = row2[\"EP\"] # name of the EP2\n",
    "        ep2_embeddings = row2[\"embeddings\"] # list of embeddings for the EP2\n",
    "        ep2_sink_code = row2[\"sink_code\"] # list of code snippets for the EP2  \n",
    "        \n",
    "        # Compute similarities\n",
    "        for i, emb1 in enumerate(ep1_embeddings):\n",
    "            for j, emb2 in enumerate(ep2_embeddings):\n",
    "                sim = torch.nn.functional.cosine_similarity(torch.tensor(emb1), torch.tensor(emb2), dim=0).item()\n",
    "                if sim > 0.8:  # Only consider pairs with similarity > 0.8\n",
    "                    similar_sink_pairs.append({\n",
    "                        \"similarity\": sim,\n",
    "                        \"ep1_code\": ep1_sink_code[i],\n",
    "                        \"ep2_id\": ep2_id,\n",
    "                        \"ep2_code\": ep2_sink_code[j]\n",
    "                    })\n",
    "    \n",
    "    # Store all similar pairs for the current EP\n",
    "    similarities[ep1_id] = similar_sink_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "for ep, similar_pairs in similarities.items():\n",
    "    if not similar_pairs:\n",
    "        print(f\"EP {ep} has no similar EPs with similarity > 0.8.\")\n",
    "    else:\n",
    "        print(f\"EP {ep} has the following similar sink code pairs with similarity > 0.8:\")\n",
    "        for pair in similar_pairs:\n",
    "            print(f\"  - Similarity: {pair['similarity']:.4f}\")\n",
    "            print(f\"    EP {ep}: \\n{pair['ep1_code']}\")\n",
    "            print(f\"    EP {pair['ep2_id']}: \\n{pair['ep2_code']}\")\n",
    "            print()\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write >0.8 similar and unique with top similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to similarities1\n"
     ]
    }
   ],
   "source": [
    "CSV_FILE = 'similarities1'\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open(CSV_FILE+ \"_all\" + \".csv\", mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write header\n",
    "    writer.writerow([\"EP1_ID\", \"EP2_ID\", \"EP1_Code\", \"EP2_Code\", \"Similarity\"])\n",
    "\n",
    "    # Iterate through the data and write each entry\n",
    "    for ep, similar_pairs in similarities.items():\n",
    "        if similar_pairs:\n",
    "            for pair in similar_pairs:\n",
    "                writer.writerow([\n",
    "                    ep,\n",
    "                    pair['ep2_id'],\n",
    "                    pair['ep1_code'].replace(\"\\n\", \" \"),  # Replace newlines for better CSV readability\n",
    "                    pair['ep2_code'].replace(\"\\n\", \" \"),  # Replace newlines for better CSV readability\n",
    "                    pair['similarity']\n",
    "                ])\n",
    "        else:\n",
    "            writer.writerow([ep, \"No similar EPs with similarity > 0.8\", \"\", \"\", \"\"])\n",
    "\n",
    "# Writing the data to a CSV file\n",
    "with open(CSV_FILE + \"_uniq\" + \".csv\", mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow([\"EP\", \"EP2_ID\", \"Max_Similarity\"])\n",
    "    \n",
    "    for ep, similar_pairs in similarities.items():\n",
    "        if not similar_pairs:\n",
    "            writer.writerow([ep, \"No similar EPs\", \"\"])\n",
    "        else:\n",
    "            max_similarity_per_ep2 = {}\n",
    "            for pair in similar_pairs:\n",
    "                ep2_id = pair['ep2_id']\n",
    "                similarity = pair['similarity']\n",
    "                if ep2_id not in max_similarity_per_ep2:\n",
    "                    max_similarity_per_ep2[ep2_id] = similarity\n",
    "                else:\n",
    "                    max_similarity_per_ep2[ep2_id] = max(max_similarity_per_ep2[ep2_id], similarity)\n",
    "            \n",
    "            # Sorting the EP2 IDs by similarity in descending order\n",
    "            sorted_ep2_ids = sorted(max_similarity_per_ep2.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Writing sorted EP2 IDs and their max similarity\n",
    "            for ep2_id, max_similarity in sorted_ep2_ids:\n",
    "                writer.writerow([ep, ep2_id, f\"{max_similarity:.4f}\"])\n",
    "\n",
    "print(f\"Data has been written to {CSV_FILE}\")\n",
    "\n",
    "\n",
    "# print(\"Data has been written to 'similarities.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep, similar_pairs in similarities.items():\n",
    "    if not similar_pairs:\n",
    "        print(f\"EP {ep} has no similar EPs with similarity > 0.8.\")\n",
    "    else:\n",
    "        print(f\"EP {ep} has the following similar sink code pairs with similarity > 0.8:\")\n",
    "        for pair in similar_pairs:\n",
    "            print(f\"  - Similarity: {pair['similarity']:.4f}\")\n",
    "            print(f\"    EP {ep}\")\n",
    "            print(f\"    EP {pair['ep2_id']}\")\n",
    "            print()\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 EP with maximum similarity value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for ep, similar_pairs in similarities.items():\n",
    "    if not similar_pairs:\n",
    "        print(f\"EP {ep} has no similar EPs with similarity > 0.8.\")\n",
    "    else:\n",
    "        print(f\"EP {ep} has the following similar sink code pairs with similarity > 0.8:\")\n",
    "        \n",
    "        max_similarity_per_ep2 = {}\n",
    "        for pair in similar_pairs:\n",
    "            ep2_id = pair['ep2_id']\n",
    "            similarity = pair['similarity']\n",
    "            if ep2_id not in max_similarity_per_ep2:\n",
    "                max_similarity_per_ep2[ep2_id] = similarity\n",
    "            else:\n",
    "                max_similarity_per_ep2[ep2_id] = max(max_similarity_per_ep2[ep2_id], similarity)\n",
    "        print(\"Unique EP2 IDs and their maximum similarity:\")\n",
    "        sorted_ep2_ids = sorted(max_similarity_per_ep2.items(), key=lambda x: x[1], reverse=True)\n",
    "        for ep2_id, max_similarity in sorted_ep2_ids:\n",
    "            print(f\"EP2 ID: {ep2_id}, Max Similarity: {max_similarity:.4f}\")\n",
    "    print(\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AMS_Df[AMS_Df[\"EP\"] == \"updateConfiguration_ActivityManagerService_2\"][\"sink_code\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unformated_APIs = AMS_Df[\"EP\"].unique()\n",
    "formated_APIs = [i.split(\"_\")[0] for i in unformated_APIs]\n",
    "# write formated apis to a csv\n",
    "pd.DataFrame(formated_APIs).to_csv(\"formated_APIs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

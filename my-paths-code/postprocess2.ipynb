{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u1/hfaheem/.conda/envs/.env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# python /u1/hfaheem/DLAndroidArtifact/my-paths-code/postprocess.py --input-dir /u1/hfaheem/DLAndroidArtifact/my-paths-code/output6 --prompt /u1/hfaheem/DLAndroidArtifact/prompts/prompt2.txt --model llama3.3 --num-ctx 25000\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.nn.functional import cosine_similarity\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import tiktoken\n",
    "import ollama\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import csv \n",
    "import argparse\n",
    "import os\n",
    "counter = 0\n",
    "\n",
    "checkpoint=\"Salesforce/codet5p-110m-embedding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(checkpoint, trust_remote_code=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_embeddings(df):\n",
    "    # Filter out rows where the embeddings column is empty or None\n",
    "    df_filtered = df[df[\"embeddings\"].apply(lambda x: x is not None and len(x) > 0)]\n",
    "    return df_filtered\n",
    "\n",
    "def get_java_code(row):\n",
    "# get the maximum depth\n",
    "    max_depth = max([x['depth'] for x in row[\"depths\"]])\n",
    "    # get all the java code for the max_depth\n",
    "    code_string = \"\"\n",
    "    path_id = 1\n",
    "    for code in row[\"depths\"]:\n",
    "        if code['depth'] == max_depth:\n",
    "            code_string += f\"This is path {path_id} for the API with depth {max_depth}:\\n\"\n",
    "            code_string += code['java_code']\n",
    "            path_id += 1\n",
    "\n",
    "    return code_string\n",
    "\n",
    "\n",
    "def get_three_java_codes(row):\n",
    "    \"\"\"Extract 3 Java code snippets: one from max depth, one from max-1, one from max-2.\n",
    "    \n",
    "    If there are not enough depths, take more from the highest available.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "\n",
    "    # Group by depth\n",
    "    depth_dict = defaultdict(list)\n",
    "    for entry in row[\"depths\"]:\n",
    "        depth_dict[entry['depth']].append(entry)\n",
    "\n",
    "    # Sort available depths in descending order\n",
    "    sorted_depths = sorted(depth_dict.keys(), reverse=True)\n",
    "    \n",
    "    selected_entries = []\n",
    "    \n",
    "    # Try to get 1 code from max, max-1, and max-2 depths\n",
    "    for i in range(3):\n",
    "        if i < len(sorted_depths):  # Check if that depth exists\n",
    "            depth = sorted_depths[i]\n",
    "            selected_entries.append(depth_dict[depth].pop(0))  # Get one entry from this depth\n",
    "    \n",
    "    # If we still need more codes, fill from highest available\n",
    "    all_remaining_entries = sum(depth_dict.values(), [])  # Flatten remaining entries\n",
    "    while len(selected_entries) < 3 and all_remaining_entries:\n",
    "        selected_entries.append(all_remaining_entries.pop(0))\n",
    "\n",
    "    # Format output with depth information\n",
    "    return '\\n\\n'.join([\n",
    "        f\"This is path {i+1} for the API with depth {entry['depth']}:\\n{entry['java_code']}\"\n",
    "        for i, entry in enumerate(selected_entries)\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "def remove_comments(json_string):\n",
    "    \"\"\"Remove single-line comments (//) from the JSON string.\"\"\"\n",
    "    return re.sub(r'//.*', '', json_string)\n",
    "\n",
    "def try_extract_and_parse(pattern, input_string):\n",
    "    \"\"\"Extract using the given regex pattern and parse JSON after cleaning comments.\"\"\"\n",
    "    json_blocks = re.findall(pattern, input_string, re.DOTALL)\n",
    "    for block in reversed(json_blocks):\n",
    "        cleaned_block = remove_comments(block).strip()\n",
    "        try:\n",
    "            return json.loads(cleaned_block)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def try_extract_boxed_json(input_string):\n",
    "    \"\"\"\n",
    "    Try to extract JSON from LaTeX-style boxed expressions of the form:\n",
    "    $\\boxed{ ... }$\n",
    "    \"\"\"\n",
    "    boxed_blocks = re.findall(r'\\$\\s*\\\\boxed\\s*\\{(.*?)\\}\\s*\\$', input_string, re.DOTALL)\n",
    "    for block in boxed_blocks:\n",
    "        cleaned_block = remove_comments(block).strip()\n",
    "        # Unescape any escaped braces if needed\n",
    "        cleaned_block = cleaned_block.replace(r'\\{', '{').replace(r'\\}', '}')\n",
    "        try:\n",
    "            return json.loads(cleaned_block)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def try_extract_curly_braces(input_string):\n",
    "    \"\"\"\n",
    "    Final fallback: Look for the first substring that starts with '{' and ends with '}'.\n",
    "    \"\"\"\n",
    "    match = re.search(r'(\\{.*\\})', input_string, re.DOTALL)\n",
    "    if match:\n",
    "        cleaned_block = remove_comments(match.group(1)).strip()\n",
    "        try:\n",
    "            return json.loads(cleaned_block)\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def extract_json_from_string(input_string):\n",
    "    \"\"\"\n",
    "    Extract JSON from a response string by trying, in order:\n",
    "      1. Code blocks explicitly tagged as JSON (```json).\n",
    "      2. Any code blocks delimited by triple backticks (```).\n",
    "      3. The entire string (if valid JSON).\n",
    "      4. LaTeX-style boxed JSON (e.g. $\\boxed{ ... }$).\n",
    "      5. The first substring that starts with '{' and ends with '}'.\n",
    "    \"\"\"\n",
    "    # 1. Try blocks tagged explicitly as JSON\n",
    "    pattern_json = r\"```json\\s*\\n(.*?)```\"\n",
    "    result = try_extract_and_parse(pattern_json, input_string)\n",
    "    if result is not None:\n",
    "        return result\n",
    "\n",
    "    # 2. Fallback: try any block delimited by triple backticks\n",
    "    pattern_any = r\"```\\s*\\n(.*?)```\"\n",
    "    result = try_extract_and_parse(pattern_any, input_string)\n",
    "    if result is not None:\n",
    "        return result\n",
    "\n",
    "    # 3. Try parsing the entire input string as JSON\n",
    "    try:\n",
    "        cleaned_input = remove_comments(input_string).strip()\n",
    "        return json.loads(cleaned_input)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # 4. Look for LaTeX-style boxed JSON (e.g. $\\boxed{ ... }$)\n",
    "    result = try_extract_boxed_json(input_string)\n",
    "    if result is not None:\n",
    "        return result\n",
    "\n",
    "    # 5. Final fallback: search for a substring that starts with '{' and ends with '}'\n",
    "    return try_extract_curly_braces(input_string)\n",
    "\n",
    "\n",
    "def get_code_embedding(code_snippet):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a given code snippet using a pre-trained model.\n",
    "\n",
    "    Parameters:\n",
    "    - code_snippet (str): The code for which embeddings are to be generated.\n",
    "    - checkpoint (str): The model checkpoint to be used for embedding. Default is Salesforce/codet5p-110m-embedding.\n",
    "    - device (str): Device to run the model on, either 'cuda' for GPU or 'cpu' for CPU. Default is 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: Embedding tensor for the input code.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs_ids = tokenizer.encode(code_snippet.strip())  # Get tokenized IDs without truncation\n",
    "\n",
    "    if len(inputs_ids) > 512:\n",
    "        print(f\"Warning: Code snippet exceeds 512 tokens ({len(inputs_ids)} tokens). It will be truncated.\")\n",
    "    inputs = tokenizer.encode(code_snippet, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        embedding = model(inputs)[0]\n",
    "    \n",
    "    return embedding.cpu()\n",
    "\n",
    "def process_json_answer(json_answer, n=float(\"inf\")):\n",
    "    global counter\n",
    "    all = []\n",
    "    if isinstance(json_answer, str):  # If it's a string, parse it\n",
    "        try:\n",
    "            json_answer = json.loads(json_answer)\n",
    "        except json.JSONDecodeError:\n",
    "            # print(\"Invalid JSON format\")\n",
    "            return []\n",
    "    try:\n",
    "        arrays = json_answer['Sinks']\n",
    "        for i, array in enumerate(arrays, 1):\n",
    "            if i > n:  # Limit the number of joins to `n`\n",
    "                break\n",
    "            joined = '\\n'.join(array)\n",
    "            all.append(joined)\n",
    "    except:    \n",
    "        return []\n",
    "    counter += 1\n",
    "    return all\n",
    "\n",
    "def calculate_embeddings(df):\n",
    "    df[\"embeddings\"] = None\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Embeddings\"):\n",
    "        code_snippets = row[\"sink_code\"]\n",
    "        embeddings = []\n",
    "        method_signature = row[\"depths\"][0]['java_code'].split(\"\\n\")[0]\n",
    "        for each in code_snippets:\n",
    "            code = f'''\n",
    "            {method_signature}\n",
    "            {each}\n",
    "            }}\n",
    "            '''\n",
    "            code_embedding = get_code_embedding(each)\n",
    "            embeddings.append(code_embedding)\n",
    "        df.at[index, \"embeddings\"] = embeddings\n",
    "\n",
    "def get_top_similar_methods(similarities, top_n=2, threshold=0.7): # change threshold here\n",
    "    filtered = [entry for entry in similarities if entry['similarity'] > threshold]\n",
    "    sorted_results = sorted(filtered, key=lambda x: x['similarity'], reverse=True)\n",
    "    top_results = sorted_results[:top_n]\n",
    "    extracted_results = [\n",
    "        {\n",
    "            'ep2_code': entry['ep2_code'],\n",
    "            'ep2_id': entry['ep2_id'],\n",
    "            'ep1_code': entry['ep1_code'],\n",
    "            'similarity': entry['similarity']\n",
    "        }\n",
    "        for entry in top_results\n",
    "    ]\n",
    "    \n",
    "    return extracted_results\n",
    "\n",
    "\n",
    "\n",
    "def get_top_similar_methods(similarities, top_n=2, threshold=0.7):\n",
    "    # Filter based on the similarity threshold\n",
    "    filtered = [entry for entry in similarities if entry['similarity'] > threshold]\n",
    "    # Sort the filtered entries in descending order by similarity\n",
    "    sorted_results = sorted(filtered, key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    unique_ep2_ids = set()\n",
    "    unique_ep2_codes = set()\n",
    "    results = []\n",
    "    \n",
    "    for entry in sorted_results:\n",
    "        # Skip if the ep2_code is already in the results\n",
    "        if entry['ep2_code'] in unique_ep2_codes:\n",
    "            continue\n",
    "        \n",
    "        # If the ep2_id is already included, add the entry (if its ep2_code is new)\n",
    "        if entry['ep2_id'] in unique_ep2_ids:\n",
    "            results.append(entry)\n",
    "            unique_ep2_codes.add(entry['ep2_code'])\n",
    "        else:\n",
    "            # This is a new unique ep2_id\n",
    "            if len(unique_ep2_ids) < top_n:\n",
    "                unique_ep2_ids.add(entry['ep2_id'])\n",
    "                results.append(entry)\n",
    "                unique_ep2_codes.add(entry['ep2_code'])\n",
    "            else:\n",
    "                # Encountering a new unique ep2_id beyond our top_n limit; break out of the loop.\n",
    "                break\n",
    "\n",
    "    # Extract and return only the desired fields\n",
    "    extracted_results = [\n",
    "        {\n",
    "            'ep2_code': entry['ep2_code'],\n",
    "            'ep2_id': entry['ep2_id'],\n",
    "            'ep1_code': entry['ep1_code'],\n",
    "            'similarity': entry['similarity']\n",
    "        }\n",
    "        for entry in results\n",
    "    ]\n",
    "    \n",
    "    return extracted_results\n",
    "\n",
    "\n",
    "def create_prompt2_string(top_similar, df, original_method, original_code, sink_code, method_name_unprocessed):\n",
    "    if not top_similar:\n",
    "        return \"No similar APIs found\"  # Return an empty string if there are no similar methods\n",
    "\n",
    "    original_class = df[df[\"method\"] == method_name_unprocessed][\"service_name\"].values[0]\n",
    "    prompt = f\"The method {original_method} in the following class {original_class} has the following code snippet:\\n\\n\"\n",
    "    prompt += f\"{original_code}\\n\"\n",
    "    prompt += f\"and the following sink code:\\n\"\n",
    "    prompt += f\"{sink_code}\\n\\n\"\n",
    "    \n",
    "    prompt += f\"The method {original_method} has the following similar APIs:\\n\\n\"\n",
    "    \n",
    "    # Group entries by unique API method (ep2_id)\n",
    "    grouped = {}\n",
    "    for entry in top_similar:\n",
    "        ep2_id = entry[\"ep2_id\"]\n",
    "        ep2_code = entry[\"ep2_code\"]\n",
    "        similarity = entry.get(\"similarity\", \"N/A\")\n",
    "        if ep2_id not in grouped:\n",
    "            grouped[ep2_id] = []\n",
    "        grouped[ep2_id].append((ep2_code, similarity))\n",
    "    \n",
    "    # Now iterate over each unique API and include all corresponding sink codes with their similarity scores\n",
    "    for ep2_id, code_entries in grouped.items():\n",
    "        # Get access control and class information from the dataframe\n",
    "        access_control = df[df[\"method\"] == ep2_id][\"access control level\"].values\n",
    "        class_name = df[df[\"method\"] == ep2_id][\"service_name\"].values[0]\n",
    "        access_control_str = access_control[0] if len(access_control) > 0 else \"Unknown\"\n",
    "        \n",
    "        prompt += f\"- API Name: {ep2_id} in the following Class: {class_name} with the following sink code entries:\\n\"\n",
    "        for code, similarity in code_entries:\n",
    "            prompt += f\"  - Similarity: {similarity}, Code:\\n{code}\\n\"\n",
    "        prompt += f\"  - Access Control Level: {access_control_str}\\n\\n\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "    \n",
    "def run_second_prompt_Ollama(method_code, model_prompt2,run, sys_prompt2,num_ctx):\n",
    "    \"\"\" runs the second prompt - extract sinks from the traces\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = method_code\n",
    "\n",
    "    response = ollama.chat(model=model_prompt2, messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': user_prompt,\n",
    "    },\n",
    "    ]\n",
    "    ,\n",
    "     options={\n",
    "        'num_ctx': num_ctx,\n",
    "        'temperature': 0.3 # Todo : Change temperature here\n",
    "    }\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"system_message\": sys_prompt2,\n",
    "        \"user_message\": user_prompt,\n",
    "        \"response\": response['message']['content']\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_80_top2(df, threshold=0.7):\n",
    "    # Preprocess all embeddings and metadata\n",
    "    all_embeddings = []\n",
    "    method_info = []  # List of tuples (method_id, code_snippet)\n",
    "    method_to_indices = {}  # Maps method to its embedding indices\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        method = row[\"method\"]\n",
    "        embeddings = row[\"embeddings\"]\n",
    "        codes = row[\"sink_code\"]\n",
    "        indices = []\n",
    "        for emb, code in zip(embeddings, codes):\n",
    "            all_embeddings.append(emb.clone().detach())\n",
    "            method_info.append((method, code))\n",
    "            indices.append(len(all_embeddings) - 1)  # Current index\n",
    "        method_to_indices[method] = indices\n",
    "\n",
    "    # Move all embeddings to GPU and normalize\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    all_embeddings = torch.stack(all_embeddings).to(device)\n",
    "    all_embeddings = torch.nn.functional.normalize(all_embeddings, p=2, dim=1)\n",
    "\n",
    "    similarities = {method: [] for method in method_to_indices.keys()}\n",
    "\n",
    "    # Process each method's embeddings against all others\n",
    "    for method in tqdm(method_to_indices, desc=\"Processing methods\"):\n",
    "        ep1_indices = method_to_indices[method]\n",
    "        if not ep1_indices:\n",
    "            continue\n",
    "\n",
    "        # Get other embeddings not belonging to this method\n",
    "        other_indices = [i for i in range(len(all_embeddings)) if method_info[i][0] != method]\n",
    "        if not other_indices:\n",
    "            continue\n",
    "\n",
    "        # Compute similarities in batches to manage memory\n",
    "        batch_size = 100000  # Adjust based on GPU memory\n",
    "        ep1_embs = all_embeddings[ep1_indices]\n",
    "        num_other = len(other_indices)\n",
    "        num_batches = (num_other + batch_size - 1) // batch_size\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start = batch_idx * batch_size\n",
    "            end = min((batch_idx + 1) * batch_size, num_other)\n",
    "            batch_other_indices = other_indices[start:end]\n",
    "            other_embs = all_embeddings[batch_other_indices]\n",
    "\n",
    "            # Compute cosine similarity matrix\n",
    "            sim_batch = torch.mm(ep1_embs, other_embs.T)\n",
    "            sim_batch = sim_batch.cpu().numpy()  # Move to CPU to save GPU memory\n",
    "\n",
    "            # Find pairs above threshold\n",
    "            rows, cols = (sim_batch > threshold).nonzero()\n",
    "            for i, j in zip(rows, cols):\n",
    "                ep1_code = method_info[ep1_indices[i]][1]\n",
    "                other_idx = batch_other_indices[j]\n",
    "                other_method, other_code = method_info[other_idx]\n",
    "                similarities[method].append({\n",
    "                    \"similarity\": sim_batch[i, j],\n",
    "                    \"ep1_code\": ep1_code,\n",
    "                    \"ep2_id\": other_method,\n",
    "                    \"ep2_code\": other_code\n",
    "                })\n",
    "\n",
    "    # Count methods with no similar pairs\n",
    "    no_similar_count = sum(1 for entries in similarities.values() if not entries)\n",
    "    print(f\"Total methods with no similar sink pairs: {no_similar_count}\")\n",
    "\n",
    "    return similarities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_csvs(similarities, CSV_FILE):\n",
    "    first_path = os.path.join(CSV_FILE, \"_allcode.csv\")\n",
    "    with open(first_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write header\n",
    "        writer.writerow([\"EP1_ID\", \"EP2_ID\", \"EP1_Code\", \"EP2_Code\", \"Similarity\"])\n",
    "\n",
    "        # Iterate through the data and write each entry\n",
    "        for ep, similar_pairs in similarities.items():\n",
    "            if similar_pairs:\n",
    "                for pair in similar_pairs:\n",
    "                    writer.writerow([\n",
    "                        ep,\n",
    "                        pair['ep2_id'],\n",
    "                        pair['ep1_code'].replace(\"\\n\", \" \"),  # Replace newlines for better CSV readability\n",
    "                        pair['ep2_code'].replace(\"\\n\", \" \"),  # Replace newlines for better CSV readability\n",
    "                        pair['similarity']\n",
    "                    ])\n",
    "            else:\n",
    "                writer.writerow([ep, \"No similar EPs with similarity > 0.8\", \"\", \"\", \"\"])\n",
    "\n",
    "    # Writing the data to a CSV file\n",
    "    second_path = os.path.join(CSV_FILE, \"_score.csv\")\n",
    "    with open(second_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header\n",
    "        writer.writerow([\"EP\", \"EP2_ID\", \"Max_Similarity\"])\n",
    "        \n",
    "        for ep, similar_pairs in similarities.items():\n",
    "            if not similar_pairs:\n",
    "                writer.writerow([ep, \"No similar EPs\", \"\"])\n",
    "            else:\n",
    "                max_similarity_per_ep2 = {}\n",
    "                for pair in similar_pairs:\n",
    "                    ep2_id = pair['ep2_id']\n",
    "                    similarity = pair['similarity']\n",
    "                    if ep2_id not in max_similarity_per_ep2:\n",
    "                        max_similarity_per_ep2[ep2_id] = similarity\n",
    "                    else:\n",
    "                        max_similarity_per_ep2[ep2_id] = max(max_similarity_per_ep2[ep2_id], similarity)\n",
    "                \n",
    "                # Sorting the EP2 IDs by similarity in descending order\n",
    "                sorted_ep2_ids = sorted(max_similarity_per_ep2.items(), key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                # Writing sorted EP2 IDs and their max similarity\n",
    "                for ep2_id, max_similarity in sorted_ep2_ids:\n",
    "                    writer.writerow([ep, ep2_id, f\"{max_similarity:.4f}\"])\n",
    "\n",
    "\n",
    "\n",
    "    third_path = os.path.join(CSV_FILE, \"_topncode\" + \".csv\")\n",
    "    with open(third_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write header\n",
    "        writer.writerow([\"EP1_ID\", \"EP2_ID\", \"EP1_Code\", \"EP2_Code\", \"Similarity\"])\n",
    "\n",
    "        # Iterate through the data and write only the top 2 most similar pairs\n",
    "        for ep, similar_pairs in similarities.items():\n",
    "            \n",
    "            if similar_pairs:\n",
    "                # Sort the pairs by similarity in descending order\n",
    "                top_pairs = sorted(similar_pairs, key=lambda x: x[\"similarity\"], reverse=True)\n",
    "                for pair in top_pairs:\n",
    "                    writer.writerow([\n",
    "                        ep,\n",
    "                        pair['ep2_id'],\n",
    "                        pair['ep1_code'].replace(\"\\n\", \" \"),  # Replace newlines for better CSV readability\n",
    "                        pair['ep2_code'].replace(\"\\n\", \" \"),  # Replace newlines for better CSV readability\n",
    "                        pair['similarity']\n",
    "                    ])\n",
    "            else:\n",
    "                writer.writerow([ep, \"No similar EPs with similarity > 0.5\", \"\", \"\", \"\"])\n",
    "\n",
    "    print(f\"Data has been written to {CSV_FILE}\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_dataframe2(df, similarities, output_folder_preprocess, model_prompt2, sys_prompt2, num_ctx):\n",
    "    df[\"json_answer2\"] = None\n",
    "    df[\"access control level predicted\"] = \"invalid\"\n",
    "    df[\"res2\"] = None  # New column to store the raw response\n",
    "\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing rows\"):\n",
    "        full_method_name = row['method']\n",
    "        method_name = row['method'].split(\"(\")[0]\n",
    "        service_name = row['service_name']\n",
    "        top_similar = get_top_similar_methods(similarities.get(full_method_name, []))\n",
    "\n",
    "        prompt = \"\"\n",
    "        json_answer = {\"access_control_level\": \"invalid\"}\n",
    "        res = {\"user_message\": \"invalid\", \"response\": \"no top_similar found\"}\n",
    "\n",
    "        if top_similar:\n",
    "            prompt = create_prompt2_string(\n",
    "                top_similar, df, method_name, get_three_java_codes(row), row[\"sink_code\"], full_method_name\n",
    "            )\n",
    "            res = run_second_prompt_Ollama(prompt, model_prompt2, True, sys_prompt2, num_ctx)\n",
    "        else:\n",
    "            res[\"response\"] = \"no top_similar found\"\n",
    "\n",
    "        # Store the raw response in df[\"res2\"]\n",
    "        df.at[index, 'res2'] = res[\"response\"]\n",
    "\n",
    "        access_control = \"invalid\"\n",
    "        json_str = \"no top_similar found\"\n",
    "\n",
    "        try:\n",
    "            json_answer = extract_json_from_string(res[\"response\"])\n",
    "            access_control = json_answer.get(\"access_control_level\", \"invalid\")\n",
    "            if not isinstance(access_control, str):\n",
    "                access_control = str(access_control)\n",
    "            json_str = json.dumps(json_answer) if json_answer else \"{}\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting JSON from response: {e}\")\n",
    "            print(f\"Method: {method_name}, Service: {service_name}\")\n",
    "            json_str = \"error extracting json\"\n",
    "\n",
    "        df.at[index, 'json_answer2'] = json_str\n",
    "        df.at[index, 'access control level predicted'] = access_control\n",
    "\n",
    "        print(f\"Method: {method_name}, Service: {service_name}, Access Control: {access_control}\")\n",
    "\n",
    "        # Define folder path and create directories \n",
    "        folder_path = os.path.join(output_folder_preprocess, service_name, method_name)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        with open(os.path.join(folder_path, 'user_message2.txt'), 'w') as f:\n",
    "            f.write(res[\"user_message\"])\n",
    "        with open(os.path.join(folder_path, 'response2.txt'), 'w') as f:\n",
    "            f.write(res[\"response\"])\n",
    "\n",
    "    # Save the updated DataFrame to a Parquet file\n",
    "    df_to_save = df.drop(columns=['embeddings'], errors='ignore')\n",
    "    df_output_file = os.path.join(output_folder_preprocess, \"android_services_methods_postprocess.parquet\")\n",
    "    df_to_save.to_parquet(df_output_file)\n",
    "    print(f\"DataFrame serialized and saved to {df_output_file}\")\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows = 3360\n",
      "row with valid json  = 6407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Embeddings:  73%|███████▎  | 2438/3360 [02:00<00:25, 36.18it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing Embeddings:  73%|███████▎  | 2442/3360 [02:00<00:34, 26.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Code snippet exceeds 512 tokens (523 tokens). It will be truncated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Embeddings: 100%|██████████| 3360/3360 [02:39<00:00, 21.05it/s]\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"input_dir\": \"/u1/hfaheem/DLAndroidArtifact/my-paths-code/output10\",\n",
    "    \"prompt\": \"/u1/hfaheem/DLAndroidArtifact/prompts/prompt2.txt\",\n",
    "    \"model\": \"llama3.3\",\n",
    "    \"num_ctx\": 25000\n",
    "}\n",
    "\n",
    "with open(args[\"prompt\"], 'r') as file:\n",
    "    sys_prompt2 = file.read()\n",
    "\n",
    "modelfile = f'''\n",
    "FROM {args[\"model\"]}\n",
    "system \"\"\"\n",
    "{sys_prompt2.strip()}\n",
    "\"\"\"\n",
    "'''\n",
    "\n",
    "model_prompt2 = \"myexample2\"\n",
    "\n",
    "ollama.create(model=model_prompt2, modelfile=modelfile)\n",
    "\n",
    "file_path = os.path.join(args[\"input_dir\"], \"android_services_methods.parquet\")\n",
    "\n",
    "df = pd.read_parquet(file_path)\n",
    "df['sink_code'] = df['json_answer'].apply(process_json_answer)\n",
    "print(f\"total rows = {len(df)}\")\n",
    "print(f\"row with valid json  = {counter}\")\n",
    "calculate_embeddings(df)\n",
    "df = remove_empty_embeddings(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing methods: 100%|██████████| 3246/3246 [00:44<00:00, 73.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total methods with no similar sink pairs: 59\n"
     ]
    }
   ],
   "source": [
    "df = remove_empty_embeddings(df)\n",
    "similarities = compute_80_top2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 3329/3329 [00:00<00:00, 18071.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# write_csvs(similarities, args[\"input_dir\"])\n",
    "\n",
    "# def process_dataframe_debug(df, similarities, output_folder_preprocess, model_prompt2, sys_prompt2, num_ctx):\n",
    "df[\"json_answer2\"] = None\n",
    "df[\"access control level predicted\"] = \"invalid\"\n",
    "df[\"res2\"] = None  # New column to store the raw response\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing rows\"):\n",
    "    full_method_name = row['method']\n",
    "    method_name = row['method'].split(\"(\")[0]\n",
    "    if method_name != \"triggerBatterySaver\":\n",
    "        continue \n",
    "    service_name = row['service_name']\n",
    "    top_similar = get_top_similar_methods(similarities.get(full_method_name, []))\n",
    "    prompt = create_prompt2_string(\n",
    "                top_similar, df, method_name, get_three_java_codes(row), row[\"sink_code\"], full_method_name\n",
    "            )\n",
    "    # print(f\"Method: {method_name}, Service: {service_name}, Top Similar: {top_similar}\")\n",
    "\n",
    "# top_similar\n",
    "prompt\n",
    "# write prompt to a text file \n",
    "with open(\"./todelete.txt\", \"w\") as f:\n",
    "    f.write(prompt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_filtered = df[df['access control level predicted'] != 'invalid']\n",
    "accuracy = (df_filtered['access control level'] == df_filtered['access control level predicted']).mean() * 100\n",
    "print(f'Overall Accuracy: {accuracy:.2f}% total rows = {len(df_filtered)}')\n",
    "\n",
    "access_levels = ['NONE', 'NORMAL', 'DANGEROUS', 'SYS_OR_SIG']\n",
    "\n",
    "for acl in access_levels:\n",
    "    subset = df_filtered[df_filtered['access control level'] == acl]\n",
    "    stats = subset['access control level predicted'].value_counts().to_frame(name='Count')\n",
    "    stats['Percentage'] = (stats['Count'] / stats['Count'].sum()) * 100\n",
    "    print(f\"\\nPredicted AC Stats for '{acl}':\")\n",
    "    print(stats.to_string())\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f684d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python ./preprocess.py --csv-dir ./input/Execution-Paths --output-dir ./tmpoutput/results --prompt ../prompts/prompt1-10.txt --model llama3.2 --num-ctx 25000\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "# import ollama\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "from openai import OpenAI\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f6e1e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_csv_files(csv_directory):\n",
    "    \"\"\"Process CSV files and structure data into DataFrame.\"\"\"\n",
    "    data_dict = {}\n",
    "    \n",
    "    for filename in os.listdir(csv_directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            service_name = os.path.splitext(filename)[0]\n",
    "            filepath = os.path.join(csv_directory, filename)\n",
    "            \n",
    "            with open(filepath, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "                reader = csv.DictReader(csvfile)\n",
    "                for row in reader:\n",
    "                    try:\n",
    "                        class_name = row['Class'].strip('\"')\n",
    "                        method = row['Method'].strip('\"')\n",
    "                        depth = row['Depth'].strip('\"')\n",
    "                        trace = row['Trace Instruction(s) ...'].strip('\"')\n",
    "                        java_code = row['Code Merged'].strip('\"')\n",
    "                        access_control_level = row['Access Control Level'].strip('\"')\n",
    "                        key = (service_name, class_name, method, access_control_level)\n",
    "                        \n",
    "                        depth_entry = {\n",
    "                            'depth': int(depth),\n",
    "                            'trace': trace,\n",
    "                            'java_code': java_code\n",
    "                        }\n",
    "                        \n",
    "                        if key not in data_dict:\n",
    "                            data_dict[key] = []\n",
    "                        data_dict[key].append(depth_entry)\n",
    "                        \n",
    "                    except KeyError as e:\n",
    "                        print(f\"Missing column in {filename}: {e}\")\n",
    "                        continue\n",
    "\n",
    "    rows = []\n",
    "    for key in data_dict:\n",
    "        service, cls, method, acl = key\n",
    "        depths = sorted(data_dict[key], key=lambda x: x['depth'])\n",
    "        rows.append({\n",
    "            'service_name': service,\n",
    "            'class': cls,\n",
    "            'method': method,\n",
    "            'depths': depths,\n",
    "            'access control level': acl\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def get_java_code(row):\n",
    "    \"\"\"Extract Java code from max depth entries.\"\"\"\n",
    "    max_depth = max([x['depth'] for x in row[\"depths\"]])\n",
    "    return '\\n'.join([f\"This is path {i+1} for the API with depth {max_depth}:\\n{code['java_code']}\"\n",
    "                     for i, code in enumerate([d for d in row[\"depths\"] if d['depth'] == max_depth])])\n",
    "\n",
    "def get_three_java_codes(row):\n",
    "    \"\"\"Extract 3 Java code snippets: one from max depth, one from max-1, one from max-2.\n",
    "    \n",
    "    If there are not enough depths, take more from the highest available.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "\n",
    "    # Group by depth\n",
    "    depth_dict = defaultdict(list)\n",
    "    for entry in row[\"depths\"]:\n",
    "        depth_dict[entry['depth']].append(entry)\n",
    "\n",
    "    # Sort available depths in descending order\n",
    "    sorted_depths = sorted(depth_dict.keys(), reverse=True)\n",
    "    \n",
    "    selected_entries = []\n",
    "    \n",
    "    # Try to get 1 code from max, max-1, and max-2 depths\n",
    "    for i in range(3):\n",
    "        if i < len(sorted_depths):  # Check if that depth exists\n",
    "            depth = sorted_depths[i]\n",
    "            selected_entries.append(depth_dict[depth].pop(0))  # Get one entry from this depth\n",
    "    \n",
    "    # If we still need more codes, fill from highest available\n",
    "    all_remaining_entries = sum(depth_dict.values(), [])  # Flatten remaining entries\n",
    "    while len(selected_entries) < 3 and all_remaining_entries:\n",
    "        selected_entries.append(all_remaining_entries.pop(0))\n",
    "\n",
    "    # Format output with depth information\n",
    "    return '\\n\\n'.join([\n",
    "        f\"This is path {i+1} for the API with depth {entry['depth']}:\\n{entry['java_code']}\"\n",
    "        for i, entry in enumerate(selected_entries)\n",
    "    ])\n",
    "\n",
    "def try_extract_and_parse(pattern, input_string, remove_comments_first=False):\n",
    "    \"\"\"Extract using the given regex pattern and parse JSON.\"\"\"\n",
    "    json_blocks = re.findall(pattern, input_string, re.DOTALL)\n",
    "    for block in reversed(json_blocks):\n",
    "        block = block.strip()\n",
    "        # First attempt without removing comments\n",
    "        try:\n",
    "            return json.loads(block)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "        # If that fails and remove_comments_first is True, try after removing comments\n",
    "        if remove_comments_first:\n",
    "            cleaned_block = remove_comments(block)\n",
    "            try:\n",
    "                return json.loads(cleaned_block)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "def try_extract_boxed_json(input_string, remove_comments_first=False):\n",
    "    \"\"\"Try to extract JSON from LaTeX-style boxed expressions of the form: $\\boxed{ ... }$.\"\"\"\n",
    "    boxed_blocks = re.findall(r'\\$\\s*\\\\boxed\\s*\\{(.*?)\\}\\s*\\$', input_string, re.DOTALL)\n",
    "    for block in boxed_blocks:\n",
    "        block = block.strip()\n",
    "\n",
    "        # First attempt without removing comments\n",
    "        try:\n",
    "            return json.loads(block)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "        # If that fails and remove_comments_first is True, try after removing comments\n",
    "        if remove_comments_first:\n",
    "            cleaned_block = remove_comments(block)\n",
    "            try:\n",
    "                return json.loads(cleaned_block)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "def try_extract_curly_braces(input_string, remove_comments_first=False):\n",
    "    \"\"\"Final fallback: Look for the first substring that starts with '{' and ends with '}'.\"\"\"\n",
    "    match = re.search(r'(\\{.*\\})', input_string, re.DOTALL)\n",
    "    if match:\n",
    "        block = match.group(1).strip()\n",
    "\n",
    "        # First attempt without removing comments\n",
    "        try:\n",
    "            return json.loads(block)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "        # If that fails and remove_comments_first is True, try after removing comments\n",
    "        if remove_comments_first:\n",
    "            cleaned_block = remove_comments(block)\n",
    "            try:\n",
    "                return json.loads(cleaned_block)\n",
    "            except json.JSONDecodeError:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def remove_comments(json_string):\n",
    "    \"\"\"Remove single-line comments (//) from the JSON string.\"\"\"\n",
    "    return re.sub(r'//.*', '', json_string)\n",
    "\n",
    "def extract_json_from_string(input_string):\n",
    "    \"\"\"\n",
    "    Extract JSON from a response string by trying, in order:\n",
    "      1. Code blocks explicitly tagged as JSON (```json).\n",
    "      2. Any code blocks delimited by triple backticks (```).\n",
    "      3. The entire string (if valid JSON).\n",
    "      4. LaTeX-style boxed JSON (e.g. $\\boxed{ ... }$).\n",
    "      5. The first substring that starts with '{' and ends with '}'.\n",
    "    Tries first without removing comments, then retries with removing comments.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Try blocks tagged explicitly as JSON\n",
    "    pattern_json = r\"```json\\s*\\n(.*?)```\"\n",
    "    result = try_extract_and_parse(pattern_json, input_string, remove_comments_first=False)\n",
    "    if result is not None:\n",
    "        return result\n",
    "\n",
    "    # Retry with comment removal\n",
    "    result = try_extract_and_parse(pattern_json, input_string, remove_comments_first=True)\n",
    "    if result is not None:\n",
    "        return result\n",
    "\n",
    "    # 2. Fallback: try any block delimited by triple backticks\n",
    "    pattern_any = r\"```\\s*\\n(.*?)```\"\n",
    "    result = try_extract_and_parse(pattern_any, input_string, remove_comments_first=False)\n",
    "    if result is not None:\n",
    "        return result\n",
    "\n",
    "    # Retry with comment removal\n",
    "    result = try_extract_and_parse(pattern_any, input_string, remove_comments_first=True)\n",
    "    if result is not None:\n",
    "        return result\n",
    "\n",
    "    # 3. Try parsing the entire input string as JSON\n",
    "    try:\n",
    "        return json.loads(input_string.strip())\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # Retry with comment removal\n",
    "    try:\n",
    "        return json.loads(remove_comments(input_string).strip())\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # 4. Look for LaTeX-style boxed JSON (e.g. $\\boxed{ ... }$)\n",
    "    result = try_extract_boxed_json(input_string, remove_comments_first=False)\n",
    "    if result is not None:\n",
    "        return result\n",
    "\n",
    "    # Retry with comment removal\n",
    "    result = try_extract_boxed_json(input_string, remove_comments_first=True)\n",
    "    if result is not None:\n",
    "        return result\n",
    "\n",
    "    # 5. Final fallback: search for a substring that starts with '{' and ends with '}'\n",
    "    result = try_extract_curly_braces(input_string, remove_comments_first=False)\n",
    "    if result is not None:\n",
    "        return result\n",
    "\n",
    "    # Retry with comment removal\n",
    "    return try_extract_curly_braces(input_string, remove_comments_first=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3a89b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1523 rows to analyze.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [27:07<00:00, 1627.66s/it]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 129\u001b[39m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved results to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 122\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# save final dataframe\u001b[39;00m\n\u001b[32m    121\u001b[39m out_file = os.path.join(args[\u001b[33m\"\u001b[39m\u001b[33moutput_dir\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mandroid_services_methods.parquet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved results to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vllm2/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vllm2/lib/python3.12/site-packages/pandas/core/frame.py:3113\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3032\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3033\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3034\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3109\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3110\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3111\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3113\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3114\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3121\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3122\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vllm2/lib/python3.12/site-packages/pandas/io/parquet.py:476\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    475\u001b[39m     partition_cols = [partition_cols]\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m    480\u001b[39m impl.write(\n\u001b[32m    481\u001b[39m     df,\n\u001b[32m    482\u001b[39m     path_or_buf,\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m     **kwargs,\n\u001b[32m    489\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/vllm2/lib/python3.12/site-packages/pandas/io/parquet.py:67\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     64\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     65\u001b[39m             error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     68\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to find a usable engine; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     69\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtried using: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfastparquet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA suitable version of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTrying to import the above resulted in these errors:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m     )\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[31mImportError\u001b[39m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "BASE_URL    = \"http://localhost:8000/v1\"\n",
    "API_KEY     = \"token123\"\n",
    "MODEL_NAME  = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
    "MAX_TOKENS  = 2500\n",
    "TEMPERATURE = 0.3\n",
    "TIMEOUT     = 3 * 24 * 3600  # 3 days\n",
    "\n",
    "# Initialize client (will be re-assigned in main if you choose)\n",
    "client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "\n",
    "def run_vllm_prompt(user_prompt, system_prompt, idx, results):\n",
    "    \"\"\"\n",
    "    Sends one chat completion to vLLM using the OpenAI client.\n",
    "    Stores a dict with system_message, user_message, and response in results[idx].\n",
    "    \"\"\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": user_prompt}\n",
    "        ],\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        temperature=TEMPERATURE,\n",
    "        stream=False,\n",
    "        timeout=TIMEOUT\n",
    "    )\n",
    "    results[idx] = {\n",
    "        \"system_message\": system_prompt,\n",
    "        \"user_message\":   user_prompt,\n",
    "        \"response\":       resp.choices[0].message.content\n",
    "    }\n",
    "\n",
    "def process_dataframe(df, output_folder, system_prompt, num_ctx, batch_size):\n",
    "    # Prepare columns\n",
    "    df['prompt1']      = df.apply(lambda row: get_three_java_codes(row), axis=1)\n",
    "    df['res1']         = None\n",
    "    df['json_answer']  = None\n",
    "\n",
    "    # Process in batches\n",
    "    for start in tqdm(range(0, len(df), batch_size), desc=\"Batches\"):\n",
    "        end = min(start + batch_size, len(df))\n",
    "        batch_idxs   = list(range(start, end))\n",
    "        batch_prompts = [df.at[i, 'prompt1'] for i in batch_idxs]\n",
    "\n",
    "        # launch threads\n",
    "        results = [None] * len(batch_prompts)\n",
    "        threads = []\n",
    "        for i, prompt in enumerate(batch_prompts):\n",
    "            t = threading.Thread(\n",
    "                target=run_vllm_prompt,\n",
    "                args=(prompt, system_prompt, i, results)\n",
    "            )\n",
    "            t.start()\n",
    "            threads.append(t)\n",
    "\n",
    "        # wait\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "\n",
    "        # collect & write out\n",
    "        for i, row_idx in enumerate(batch_idxs):\n",
    "            res = results[i]\n",
    "            df.at[row_idx, 'res1']        = res['response']\n",
    "            parsed = extract_json_from_string(res['response'])\n",
    "            df.at[row_idx, 'json_answer'] = json.dumps(parsed) if isinstance(parsed, (dict, list)) else str(parsed)\n",
    "\n",
    "            # save files\n",
    "            method_name  = df.at[row_idx, 'method'].split(\"(\")[0]\n",
    "            service_name = df.at[row_idx, 'service_name']\n",
    "            folder = os.path.join(output_folder, service_name, method_name)\n",
    "            os.makedirs(folder, exist_ok=True)\n",
    "            for fname, content in [\n",
    "                ('system_message.txt', res['system_message']),\n",
    "                ('user_message.txt',   res['user_message']),\n",
    "                ('response.txt',       res['response'])\n",
    "            ]:\n",
    "                with open(os.path.join(folder, fname), 'w') as f:\n",
    "                    f.write(content)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Batch vLLM analysis of Java methods')\n",
    "    # Define variables for testing in Jupyter Notebook\n",
    "    args = {\n",
    "        \"csv_dir\": \"/u1/hfaheem/DLAndroidArtifact/my-paths-code/input/Execution-Paths\", \n",
    "        \"output_dir\": \"./output_results_vllm_Qwen2.5-Coder\",  \n",
    "        \"prompt\": \"/u1/hfaheem/DLAndroidArtifact/prompts/prompt1-11.txt\",  \n",
    "        \"num_ctx\": 25000,  # Context window size (ignored by vLLM)\n",
    "        \"batch_size\": 150  # Number of prompts per VLLM batch\n",
    "    }\n",
    "\n",
    "    # load system prompt\n",
    "    with open(args[\"prompt\"], 'r') as f:\n",
    "        system_prompt = f.read().strip()\n",
    "\n",
    "    # re-init client in case you want to override via env or args\n",
    "    global client\n",
    "    client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "\n",
    "    # load your CSVs and flatten into one df\n",
    "    df = process_csv_files(args[\"csv_dir\"])\n",
    "    print(f\"Loaded {len(df)} rows to analyze.\")\n",
    "    \n",
    "    df = df[:10]\n",
    "    # run batched inference\n",
    "    process_dataframe(\n",
    "        df,\n",
    "        output_folder=args[\"output_dir\"],\n",
    "        system_prompt=system_prompt,\n",
    "        num_ctx=args[\"num_ctx\"],\n",
    "        batch_size=args[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    # save final dataframe\n",
    "    out_file = os.path.join(args[\"output_dir\"], \"android_services_methods.parquet\")\n",
    "    df.to_parquet(out_file)\n",
    "    print(f\"Saved results to {out_file}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64283488",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Downloading pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-19.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa486a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
